{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data augmentation : 1) 데이터 좌우반전(2배). 2) size 4만큼 패딩 후 32의 크기로 random cropping\n",
    "transforms_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomCrop(32, padding=4),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])   #평균값은 미리 구해놓은 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR10(root='./dataset/', train=True, transform=transforms_train, download=True)\n",
    "train_data, val_data = torch.utils.data.random_split(train_val_data, [40000,10000])\n",
    "test_data = datasets.CIFAR10(root=\"./dataset/\", train=False, transform=transforms_test, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1st image:\n",
      "tensor([[[-1.7118, -1.6731, -1.6924,  ..., -1.1690, -1.4211, -1.2854],\n",
      "         [-1.8669, -1.6537, -1.5567,  ..., -1.3823, -1.1690, -1.0527],\n",
      "         [-1.6343, -1.7118, -1.4986,  ..., -1.2854, -1.0915, -0.9364],\n",
      "         ...,\n",
      "         [ 0.4205,  0.5756,  0.7307,  ...,  1.1959,  1.3122,  1.1765],\n",
      "         [ 0.5368,  0.6338,  0.7694,  ...,  1.0990,  1.1184,  0.9245],\n",
      "         [ 0.5368,  0.7113,  0.7888,  ...,  1.0796,  0.8858,  0.7501]],\n",
      "\n",
      "        [[-1.6709, -1.5922, -1.5922,  ..., -0.9432, -1.2186, -1.1006],\n",
      "         [-1.7889, -1.5529, -1.4349,  ..., -1.2382, -0.9629, -0.8252],\n",
      "         [-1.4939, -1.5529, -1.3169,  ..., -1.1792, -0.8646, -0.6482],\n",
      "         ...,\n",
      "         [ 1.0824,  1.1808,  1.2988,  ...,  1.8101,  1.9871,  1.9281],\n",
      "         [ 1.2004,  1.2398,  1.3381,  ...,  1.7708,  1.8495,  1.7118],\n",
      "         [ 1.2004,  1.2988,  1.3774,  ...,  1.7118,  1.5741,  1.5151]],\n",
      "\n",
      "        [[-1.0898, -1.1873, -1.3239,  ..., -1.2069, -1.5385, -1.3434],\n",
      "         [-1.3434, -1.2459, -1.2459,  ..., -1.3434, -1.2069, -1.1093],\n",
      "         [-1.2264, -1.4020, -1.2459,  ..., -1.1288, -1.0313, -0.9532],\n",
      "         ...,\n",
      "         [-0.0362,  0.0808,  0.1394,  ...,  0.4710,  0.6076,  0.5686],\n",
      "         [ 0.0418,  0.1394,  0.1394,  ...,  0.4125,  0.4515,  0.3345],\n",
      "         [ 0.0028,  0.1979,  0.1003,  ...,  0.3540,  0.1979,  0.1589]]])\n",
      "Shape of this image\t: torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "print(\"The 1st image:\")\n",
    "print(image)\n",
    "print('Shape of this image\\t:', image.shape)\n",
    "plt.imshow(np.transpose(image, (1, 2, 0)))\n",
    "plt.title('Label:%s' % classes[label])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5_model(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_out1): Linear(in_features=400, out_features=128, bias=True)\n",
      "  (fc_out2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#(in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t = 1, \n",
    "# padding: _size_2_t | str = 0, dilation: _size_2_t = 1, groups: int = 1, bias: bool = True,\n",
    "# padding_mode: str = 'zeros', device=None, dtype=None) -> None\n",
    "#Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "\n",
    "#(kernel_size: _size_any_t, stride: _size_any_t | None = None, padding: _size_any_t = 0, dilation: _size_any_t = 1, \n",
    "# return_indices: bool = False, ceil_mode: bool = False) -> None\n",
    "#Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "\n",
    "class LeNet5_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5_model,self).__init__()\n",
    "        #(3,32,32)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=5,stride = 1, padding= 0)\n",
    "        #(6,28,28)\n",
    "        self.pooling1 = nn.MaxPool2d(kernel_size = 2, stride= 2)\n",
    "        #(6,14,14)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride = 1, padding= 0)\n",
    "        #(16,10,10)\n",
    "        self.pooling2 = nn.MaxPool2d(kernel_size = 2, stride= 2)\n",
    "        #(16,5,5)\n",
    "        self.fc_out1 = nn.Linear(400,128)\n",
    "        self.fc_out2 = nn.Linear(128,10)         \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pooling1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pooling2(x)\n",
    "        x = x.view(-1,400)\n",
    "        x = F.relu(self.fc_out1(x))\n",
    "        x = self.fc_out2(x)\n",
    "        return x\n",
    "        \n",
    "model = LeNet5_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(16,5,5)\n",
    "a = a.view(-1,400)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5_model(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_out1): Linear(in_features=400, out_features=128, bias=True)\n",
       "  (fc_out2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batch_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_batch_loader = DataLoader(val_data, batch_size, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lossfunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/0], Loss: 1.7538\n",
      "Epoch [1/50], Step [200/0], Loss: 1.4322\n",
      "Epoch [1/50], Step [300/0], Loss: 1.6279\n",
      "Epoch [2/50], Step [100/0], Loss: 1.4779\n",
      "Epoch [2/50], Step [200/0], Loss: 1.3997\n",
      "Epoch [2/50], Step [300/0], Loss: 1.3633\n",
      "Epoch [3/50], Step [100/0], Loss: 1.2958\n",
      "Epoch [3/50], Step [200/0], Loss: 1.3745\n",
      "Epoch [3/50], Step [300/0], Loss: 1.2212\n",
      "Epoch [4/50], Step [100/0], Loss: 1.2638\n",
      "Epoch [4/50], Step [200/0], Loss: 1.2588\n",
      "Epoch [4/50], Step [300/0], Loss: 1.1287\n",
      "Epoch [5/50], Step [100/0], Loss: 1.2703\n",
      "Epoch [5/50], Step [200/0], Loss: 1.3286\n",
      "Epoch [5/50], Step [300/0], Loss: 1.1902\n",
      "Epoch [6/50], Step [100/0], Loss: 1.0957\n",
      "Epoch [6/50], Step [200/0], Loss: 1.2125\n",
      "Epoch [6/50], Step [300/0], Loss: 0.9360\n",
      "Epoch [7/50], Step [100/0], Loss: 1.0519\n",
      "Epoch [7/50], Step [200/0], Loss: 1.0698\n",
      "Epoch [7/50], Step [300/0], Loss: 1.1524\n",
      "Epoch [8/50], Step [100/0], Loss: 1.0133\n",
      "Epoch [8/50], Step [200/0], Loss: 1.1798\n",
      "Epoch [8/50], Step [300/0], Loss: 1.0361\n",
      "Epoch [9/50], Step [100/0], Loss: 1.1088\n",
      "Epoch [9/50], Step [200/0], Loss: 1.0079\n",
      "Epoch [9/50], Step [300/0], Loss: 1.1169\n",
      "Epoch [10/50], Step [100/0], Loss: 0.9739\n",
      "Epoch [10/50], Step [200/0], Loss: 1.1955\n",
      "Epoch [10/50], Step [300/0], Loss: 1.0734\n",
      "Epoch [11/50], Step [100/0], Loss: 0.8209\n",
      "Epoch [11/50], Step [200/0], Loss: 0.9162\n",
      "Epoch [11/50], Step [300/0], Loss: 0.9524\n",
      "Epoch [12/50], Step [100/0], Loss: 1.0434\n",
      "Epoch [12/50], Step [200/0], Loss: 0.9784\n",
      "Epoch [12/50], Step [300/0], Loss: 0.9797\n",
      "Epoch [13/50], Step [100/0], Loss: 0.9606\n",
      "Epoch [13/50], Step [200/0], Loss: 0.9469\n",
      "Epoch [13/50], Step [300/0], Loss: 0.8995\n",
      "Epoch [14/50], Step [100/0], Loss: 0.7805\n",
      "Epoch [14/50], Step [200/0], Loss: 0.9688\n",
      "Epoch [14/50], Step [300/0], Loss: 0.8058\n",
      "Epoch [15/50], Step [100/0], Loss: 0.8254\n",
      "Epoch [15/50], Step [200/0], Loss: 0.9316\n",
      "Epoch [15/50], Step [300/0], Loss: 0.8529\n",
      "Epoch [16/50], Step [100/0], Loss: 0.9291\n",
      "Epoch [16/50], Step [200/0], Loss: 1.0837\n",
      "Epoch [16/50], Step [300/0], Loss: 0.8443\n",
      "Epoch [17/50], Step [100/0], Loss: 0.9858\n",
      "Epoch [17/50], Step [200/0], Loss: 0.9524\n",
      "Epoch [17/50], Step [300/0], Loss: 0.7162\n",
      "Epoch [18/50], Step [100/0], Loss: 0.7487\n",
      "Epoch [18/50], Step [200/0], Loss: 0.8171\n",
      "Epoch [18/50], Step [300/0], Loss: 0.8519\n",
      "Epoch [19/50], Step [100/0], Loss: 0.8909\n",
      "Epoch [19/50], Step [200/0], Loss: 0.7104\n",
      "Epoch [19/50], Step [300/0], Loss: 0.7694\n",
      "Epoch [20/50], Step [100/0], Loss: 0.7874\n",
      "Epoch [20/50], Step [200/0], Loss: 0.8809\n",
      "Epoch [20/50], Step [300/0], Loss: 0.8867\n",
      "Epoch [21/50], Step [100/0], Loss: 0.8767\n",
      "Epoch [21/50], Step [200/0], Loss: 0.8870\n",
      "Epoch [21/50], Step [300/0], Loss: 0.8221\n",
      "Epoch [22/50], Step [100/0], Loss: 0.7602\n",
      "Epoch [22/50], Step [200/0], Loss: 0.7792\n",
      "Epoch [22/50], Step [300/0], Loss: 0.8088\n",
      "Epoch [23/50], Step [100/0], Loss: 0.7297\n",
      "Epoch [23/50], Step [200/0], Loss: 0.8313\n",
      "Epoch [23/50], Step [300/0], Loss: 0.9842\n",
      "Epoch [24/50], Step [100/0], Loss: 0.8489\n",
      "Epoch [24/50], Step [200/0], Loss: 0.7811\n",
      "Epoch [24/50], Step [300/0], Loss: 0.6535\n",
      "Epoch [25/50], Step [100/0], Loss: 0.6312\n",
      "Epoch [25/50], Step [200/0], Loss: 0.6904\n",
      "Epoch [25/50], Step [300/0], Loss: 0.7198\n",
      "Epoch [26/50], Step [100/0], Loss: 0.7403\n",
      "Epoch [26/50], Step [200/0], Loss: 0.8151\n",
      "Epoch [26/50], Step [300/0], Loss: 0.7478\n",
      "Epoch [27/50], Step [100/0], Loss: 0.7628\n",
      "Epoch [27/50], Step [200/0], Loss: 0.6126\n",
      "Epoch [27/50], Step [300/0], Loss: 0.8079\n",
      "Epoch [28/50], Step [100/0], Loss: 0.6481\n",
      "Epoch [28/50], Step [200/0], Loss: 0.7337\n",
      "Epoch [28/50], Step [300/0], Loss: 0.8106\n",
      "Epoch [29/50], Step [100/0], Loss: 0.4926\n",
      "Epoch [29/50], Step [200/0], Loss: 0.8396\n",
      "Epoch [29/50], Step [300/0], Loss: 0.7922\n",
      "Epoch [30/50], Step [100/0], Loss: 0.6742\n",
      "Epoch [30/50], Step [200/0], Loss: 0.7599\n",
      "Epoch [30/50], Step [300/0], Loss: 0.7930\n",
      "Epoch [31/50], Step [100/0], Loss: 0.7225\n",
      "Epoch [31/50], Step [200/0], Loss: 0.5915\n",
      "Epoch [31/50], Step [300/0], Loss: 0.7644\n",
      "Epoch [32/50], Step [100/0], Loss: 0.7936\n",
      "Epoch [32/50], Step [200/0], Loss: 0.6732\n",
      "Epoch [32/50], Step [300/0], Loss: 0.7135\n",
      "Epoch [33/50], Step [100/0], Loss: 0.6903\n",
      "Epoch [33/50], Step [200/0], Loss: 0.6679\n",
      "Epoch [33/50], Step [300/0], Loss: 0.7767\n",
      "Epoch [34/50], Step [100/0], Loss: 0.5952\n",
      "Epoch [34/50], Step [200/0], Loss: 0.5641\n",
      "Epoch [34/50], Step [300/0], Loss: 0.6171\n",
      "Epoch [35/50], Step [100/0], Loss: 0.6708\n",
      "Epoch [35/50], Step [200/0], Loss: 0.6485\n",
      "Epoch [35/50], Step [300/0], Loss: 0.7335\n",
      "Epoch [36/50], Step [100/0], Loss: 0.7409\n",
      "Epoch [36/50], Step [200/0], Loss: 0.6590\n",
      "Epoch [36/50], Step [300/0], Loss: 0.7296\n",
      "Epoch [37/50], Step [100/0], Loss: 0.7741\n",
      "Epoch [37/50], Step [200/0], Loss: 0.6117\n",
      "Epoch [37/50], Step [300/0], Loss: 0.6707\n",
      "Epoch [38/50], Step [100/0], Loss: 0.8135\n",
      "Epoch [38/50], Step [200/0], Loss: 0.6800\n",
      "Epoch [38/50], Step [300/0], Loss: 0.6603\n",
      "Epoch [39/50], Step [100/0], Loss: 0.5828\n",
      "Epoch [39/50], Step [200/0], Loss: 0.6183\n",
      "Epoch [39/50], Step [300/0], Loss: 0.6483\n",
      "Epoch [40/50], Step [100/0], Loss: 0.4369\n",
      "Epoch [40/50], Step [200/0], Loss: 0.6422\n",
      "Epoch [40/50], Step [300/0], Loss: 0.6394\n",
      "Epoch [41/50], Step [100/0], Loss: 0.7416\n",
      "Epoch [41/50], Step [200/0], Loss: 0.5085\n",
      "Epoch [41/50], Step [300/0], Loss: 0.7029\n",
      "Epoch [42/50], Step [100/0], Loss: 0.5514\n",
      "Epoch [42/50], Step [200/0], Loss: 0.5817\n",
      "Epoch [42/50], Step [300/0], Loss: 0.8329\n",
      "Epoch [43/50], Step [100/0], Loss: 0.6148\n",
      "Epoch [43/50], Step [200/0], Loss: 0.6502\n",
      "Epoch [43/50], Step [300/0], Loss: 0.6640\n",
      "Epoch [44/50], Step [100/0], Loss: 0.6540\n",
      "Epoch [44/50], Step [200/0], Loss: 0.7167\n",
      "Epoch [44/50], Step [300/0], Loss: 0.6078\n",
      "Epoch [45/50], Step [100/0], Loss: 0.5969\n",
      "Epoch [45/50], Step [200/0], Loss: 0.6207\n",
      "Epoch [45/50], Step [300/0], Loss: 0.6065\n",
      "Epoch [46/50], Step [100/0], Loss: 0.7322\n",
      "Epoch [46/50], Step [200/0], Loss: 0.5191\n",
      "Epoch [46/50], Step [300/0], Loss: 0.5816\n",
      "Epoch [47/50], Step [100/0], Loss: 0.6639\n",
      "Epoch [47/50], Step [200/0], Loss: 0.5561\n",
      "Epoch [47/50], Step [300/0], Loss: 0.7153\n",
      "Epoch [48/50], Step [100/0], Loss: 0.5967\n",
      "Epoch [48/50], Step [200/0], Loss: 0.7004\n",
      "Epoch [48/50], Step [300/0], Loss: 0.5165\n",
      "Epoch [49/50], Step [100/0], Loss: 0.5596\n",
      "Epoch [49/50], Step [200/0], Loss: 0.5539\n",
      "Epoch [49/50], Step [300/0], Loss: 0.6134\n",
      "Epoch [50/50], Step [100/0], Loss: 0.6372\n",
      "Epoch [50/50], Step [200/0], Loss: 0.6237\n",
      "Epoch [50/50], Step [300/0], Loss: 0.6960\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "total_step = 0\n",
    "max_value = 0\n",
    "for epoch in range(50):\n",
    "  model.train()\n",
    "  for i, (images, labels) in enumerate(train_batch_loader):\n",
    "    # 이미지와 정답(label)을 device로 올림\n",
    "    images = images.to(device) \n",
    "    labels = labels.to(device)\n",
    "    # Feedforward 과정\n",
    "    outputs = model(images)\n",
    "    # Loss 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad() # iteration 마다 gradient를 0으로 초기화\n",
    "    loss.backward() # 가중치 w에 대해 loss를 미분\n",
    "    optimizer.step() # 가중치들을 업데이트\n",
    "    if (i+1) % 100 == 0:\n",
    "        loss_arr.append(loss.item()) \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, 50, (i+1)/300, 100, loss.item()))\n",
    "  '''\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    acc = evaluation(dev_loader)\n",
    "    if max_value < acc :\n",
    "      max_value = acc\n",
    "      print(\"max dev accuracy: \", max_value)\n",
    "      torch.save(model.state_dict(), 'model.ckpt')\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Start training...\n",
      "========================================\n",
      "epoch: 1 / global_steps: 313\n",
      "training dataset average loss: 0.625\n",
      "training_time: 0.13 minutes\n",
      "validation dataset accuracy: 64.58\n",
      "========================================\n",
      "epoch: 2 / global_steps: 626\n",
      "training dataset average loss: 0.614\n",
      "training_time: 0.30 minutes\n",
      "validation dataset accuracy: 65.03\n",
      "========================================\n",
      "epoch: 3 / global_steps: 939\n",
      "training dataset average loss: 0.612\n",
      "training_time: 0.46 minutes\n",
      "validation dataset accuracy: 65.52\n",
      "========================================\n",
      "epoch: 4 / global_steps: 1252\n",
      "training dataset average loss: 0.601\n",
      "training_time: 0.63 minutes\n",
      "validation dataset accuracy: 65.34\n",
      "========================================\n",
      "epoch: 5 / global_steps: 1565\n",
      "training dataset average loss: 0.605\n",
      "training_time: 0.80 minutes\n",
      "validation dataset accuracy: 65.17\n",
      "========================================\n",
      "epoch: 6 / global_steps: 1878\n",
      "training dataset average loss: 0.599\n",
      "training_time: 0.97 minutes\n",
      "validation dataset accuracy: 65.54\n",
      "========================================\n",
      "epoch: 7 / global_steps: 2191\n",
      "training dataset average loss: 0.599\n",
      "training_time: 1.14 minutes\n",
      "validation dataset accuracy: 65.17\n",
      "========================================\n",
      "epoch: 8 / global_steps: 2504\n",
      "training dataset average loss: 0.589\n",
      "training_time: 1.30 minutes\n",
      "validation dataset accuracy: 65.03\n",
      "========================================\n",
      "epoch: 9 / global_steps: 2817\n",
      "training dataset average loss: 0.586\n",
      "training_time: 1.47 minutes\n",
      "validation dataset accuracy: 64.50\n",
      "========================================\n",
      "epoch: 10 / global_steps: 3130\n",
      "training dataset average loss: 0.582\n",
      "training_time: 1.64 minutes\n",
      "validation dataset accuracy: 64.82\n",
      "========================================\n",
      "epoch: 11 / global_steps: 3443\n",
      "training dataset average loss: 0.578\n",
      "training_time: 1.81 minutes\n",
      "validation dataset accuracy: 65.42\n",
      "========================================\n",
      "epoch: 12 / global_steps: 3756\n",
      "training dataset average loss: 0.573\n",
      "training_time: 1.98 minutes\n",
      "validation dataset accuracy: 65.45\n",
      "========================================\n",
      "epoch: 13 / global_steps: 4069\n",
      "training dataset average loss: 0.570\n",
      "training_time: 2.15 minutes\n",
      "validation dataset accuracy: 64.49\n",
      "========================================\n",
      "epoch: 14 / global_steps: 4382\n",
      "training dataset average loss: 0.568\n",
      "training_time: 2.32 minutes\n",
      "validation dataset accuracy: 65.31\n",
      "========================================\n",
      "epoch: 15 / global_steps: 4695\n",
      "training dataset average loss: 0.566\n",
      "training_time: 2.49 minutes\n",
      "validation dataset accuracy: 64.38\n",
      "========================================\n",
      "epoch: 16 / global_steps: 5008\n",
      "training dataset average loss: 0.557\n",
      "training_time: 2.66 minutes\n",
      "validation dataset accuracy: 64.79\n",
      "========================================\n",
      "epoch: 17 / global_steps: 5321\n",
      "training dataset average loss: 0.552\n",
      "training_time: 2.84 minutes\n",
      "validation dataset accuracy: 65.03\n",
      "========================================\n",
      "epoch: 18 / global_steps: 5634\n",
      "training dataset average loss: 0.550\n",
      "training_time: 3.02 minutes\n",
      "validation dataset accuracy: 65.03\n",
      "========================================\n",
      "epoch: 19 / global_steps: 5947\n",
      "training dataset average loss: 0.554\n",
      "training_time: 3.19 minutes\n",
      "validation dataset accuracy: 64.02\n",
      "========================================\n",
      "epoch: 20 / global_steps: 6260\n",
      "training dataset average loss: 0.546\n",
      "training_time: 3.36 minutes\n",
      "validation dataset accuracy: 65.10\n",
      "========================================\n",
      "epoch: 21 / global_steps: 6573\n",
      "training dataset average loss: 0.542\n",
      "training_time: 3.54 minutes\n",
      "validation dataset accuracy: 64.59\n",
      "========================================\n",
      "epoch: 22 / global_steps: 6886\n",
      "training dataset average loss: 0.544\n",
      "training_time: 3.71 minutes\n",
      "validation dataset accuracy: 64.09\n",
      "========================================\n",
      "epoch: 23 / global_steps: 7199\n",
      "training dataset average loss: 0.542\n",
      "training_time: 3.89 minutes\n",
      "validation dataset accuracy: 65.04\n",
      "========================================\n",
      "epoch: 24 / global_steps: 7512\n",
      "training dataset average loss: 0.532\n",
      "training_time: 4.06 minutes\n",
      "validation dataset accuracy: 64.00\n",
      "========================================\n",
      "epoch: 25 / global_steps: 7825\n",
      "training dataset average loss: 0.534\n",
      "training_time: 4.24 minutes\n",
      "validation dataset accuracy: 63.89\n",
      "========================================\n",
      "epoch: 26 / global_steps: 8138\n",
      "training dataset average loss: 0.524\n",
      "training_time: 4.41 minutes\n",
      "validation dataset accuracy: 64.00\n",
      "========================================\n",
      "epoch: 27 / global_steps: 8451\n",
      "training dataset average loss: 0.532\n",
      "training_time: 4.59 minutes\n",
      "validation dataset accuracy: 64.67\n",
      "========================================\n",
      "epoch: 28 / global_steps: 8764\n",
      "training dataset average loss: 0.521\n",
      "training_time: 4.76 minutes\n",
      "validation dataset accuracy: 64.73\n",
      "========================================\n",
      "epoch: 29 / global_steps: 9077\n",
      "training dataset average loss: 0.530\n",
      "training_time: 4.93 minutes\n",
      "validation dataset accuracy: 64.99\n",
      "========================================\n",
      "epoch: 30 / global_steps: 9390\n",
      "training dataset average loss: 0.525\n",
      "training_time: 5.11 minutes\n",
      "validation dataset accuracy: 64.85\n",
      "========================================\n",
      "epoch: 31 / global_steps: 9703\n",
      "training dataset average loss: 0.514\n",
      "training_time: 5.29 minutes\n",
      "validation dataset accuracy: 64.83\n",
      "========================================\n",
      "epoch: 32 / global_steps: 10016\n",
      "training dataset average loss: 0.515\n",
      "training_time: 5.47 minutes\n",
      "validation dataset accuracy: 64.37\n",
      "========================================\n",
      "epoch: 33 / global_steps: 10329\n",
      "training dataset average loss: 0.512\n",
      "training_time: 5.66 minutes\n",
      "validation dataset accuracy: 64.28\n",
      "========================================\n",
      "epoch: 34 / global_steps: 10642\n",
      "training dataset average loss: 0.507\n",
      "training_time: 5.83 minutes\n",
      "validation dataset accuracy: 63.94\n",
      "========================================\n",
      "epoch: 35 / global_steps: 10955\n",
      "training dataset average loss: 0.502\n",
      "training_time: 6.00 minutes\n",
      "validation dataset accuracy: 63.56\n",
      "========================================\n",
      "epoch: 36 / global_steps: 11268\n",
      "training dataset average loss: 0.501\n",
      "training_time: 6.17 minutes\n",
      "validation dataset accuracy: 63.22\n",
      "========================================\n",
      "epoch: 37 / global_steps: 11581\n",
      "training dataset average loss: 0.501\n",
      "training_time: 6.35 minutes\n",
      "validation dataset accuracy: 64.34\n",
      "========================================\n",
      "epoch: 38 / global_steps: 11894\n",
      "training dataset average loss: 0.498\n",
      "training_time: 6.52 minutes\n",
      "validation dataset accuracy: 63.99\n",
      "========================================\n",
      "epoch: 39 / global_steps: 12207\n",
      "training dataset average loss: 0.491\n",
      "training_time: 6.70 minutes\n",
      "validation dataset accuracy: 64.23\n",
      "========================================\n",
      "epoch: 40 / global_steps: 12520\n",
      "training dataset average loss: 0.493\n",
      "training_time: 6.87 minutes\n",
      "validation dataset accuracy: 63.88\n",
      "========================================\n",
      "epoch: 41 / global_steps: 12833\n",
      "training dataset average loss: 0.492\n",
      "training_time: 7.04 minutes\n",
      "validation dataset accuracy: 64.57\n",
      "========================================\n",
      "epoch: 42 / global_steps: 13146\n",
      "training dataset average loss: 0.488\n",
      "training_time: 7.22 minutes\n",
      "validation dataset accuracy: 63.58\n",
      "========================================\n",
      "epoch: 43 / global_steps: 13459\n",
      "training dataset average loss: 0.488\n",
      "training_time: 7.39 minutes\n",
      "validation dataset accuracy: 63.71\n",
      "========================================\n",
      "epoch: 44 / global_steps: 13772\n",
      "training dataset average loss: 0.486\n",
      "training_time: 7.56 minutes\n",
      "validation dataset accuracy: 64.31\n",
      "========================================\n",
      "epoch: 45 / global_steps: 14085\n",
      "training dataset average loss: 0.483\n",
      "training_time: 7.74 minutes\n",
      "validation dataset accuracy: 63.96\n",
      "========================================\n",
      "epoch: 46 / global_steps: 14398\n",
      "training dataset average loss: 0.485\n",
      "training_time: 7.91 minutes\n",
      "validation dataset accuracy: 63.42\n",
      "========================================\n",
      "epoch: 47 / global_steps: 14711\n",
      "training dataset average loss: 0.476\n",
      "training_time: 8.09 minutes\n",
      "validation dataset accuracy: 63.19\n",
      "Training finished.\n",
      "========================================\n",
      "epoch: 48 / global_steps: 15000\n",
      "training dataset average loss: 0.477\n",
      "training_time: 8.25 minutes\n",
      "validation dataset accuracy: 63.40\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "highest_val_acc = 0\n",
    "loss_list = []\n",
    "val_acc_list = []\n",
    "global_steps = 0\n",
    "epoch = 0\n",
    "\n",
    "print('========================================')\n",
    "print(\"Start training...\")\n",
    "while True:\n",
    "    train_loss = 0\n",
    "    train_batch_cnt = 0\n",
    "    model.train()\n",
    "    for img, label in train_batch_loader:\n",
    "        global_steps += 1\n",
    "        # img.shape: [128,3,32,32]\n",
    "        # label.shape: [128]\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        train_batch_cnt += 1\n",
    "\n",
    "        #loss_list.append(loss)        \n",
    "\n",
    "        if global_steps >= 15000:\n",
    "            print(\"Training finished.\")\n",
    "            break\n",
    "\n",
    "    ave_loss = train_loss / train_batch_cnt\n",
    "    loss_list.append(ave_loss)\n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    print('========================================')\n",
    "    print(\"epoch:\", epoch + 1, \"/ global_steps:\", global_steps)\n",
    "    print(\"training dataset average loss: %.3f\" % ave_loss)\n",
    "    print(\"training_time: %.2f minutes\" % training_time)\n",
    "\n",
    "    # validation (for early stopping)\n",
    "    correct_cnt = 0\n",
    "    model.eval()\n",
    "    for img, label in val_batch_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = model.forward(img)\n",
    "        _, top_pred = torch.topk(pred, k=1, dim=-1)\n",
    "        top_pred = top_pred.squeeze(dim=1)\n",
    "        correct_cnt += int(torch.sum(top_pred == label))\n",
    "\n",
    "    val_acc = correct_cnt / len(val_data) * 100\n",
    "    print(\"validation dataset accuracy: %.2f\" % val_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    if val_acc > highest_val_acc:\n",
    "        save_path = './' + str(epoch + 1) + '.pth'\n",
    "        # 위와 같이 저장 위치를 바꾸어 가며 각 setting의 epoch마다의 state를 저장할 것.\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict()},\n",
    "                    save_path)\n",
    "        highest_val_acc = val_acc\n",
    "    epoch += 1\n",
    "    if global_steps >= 15000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64.58,\n",
       " 65.03,\n",
       " 65.52,\n",
       " 65.34,\n",
       " 65.16999999999999,\n",
       " 65.53999999999999,\n",
       " 65.16999999999999,\n",
       " 65.03,\n",
       " 64.5,\n",
       " 64.82,\n",
       " 65.42,\n",
       " 65.45,\n",
       " 64.49000000000001,\n",
       " 65.31,\n",
       " 64.38000000000001,\n",
       " 64.79,\n",
       " 65.03,\n",
       " 65.03,\n",
       " 64.02,\n",
       " 65.10000000000001,\n",
       " 64.59,\n",
       " 64.09,\n",
       " 65.03999999999999,\n",
       " 64.0,\n",
       " 63.89,\n",
       " 64.0,\n",
       " 64.67,\n",
       " 64.73,\n",
       " 64.99000000000001,\n",
       " 64.85,\n",
       " 64.83,\n",
       " 64.37,\n",
       " 64.28,\n",
       " 63.94,\n",
       " 63.56,\n",
       " 63.22,\n",
       " 64.34,\n",
       " 63.99,\n",
       " 64.23,\n",
       " 63.88,\n",
       " 64.57000000000001,\n",
       " 63.580000000000005,\n",
       " 63.71,\n",
       " 64.31,\n",
       " 63.959999999999994,\n",
       " 63.42,\n",
       " 63.190000000000005,\n",
       " 63.4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_list"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bdc51bfe3c094edef4832d44e2732f9ad87140ae1289ea685f070d0d674a5d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
